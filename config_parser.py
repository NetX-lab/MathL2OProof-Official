import configargparse

# Argument Parsing
parser = configargparse.get_arg_parser(
    description='Configurations for L2O experiements')

parser.add('-c', '--config', is_config_file=True, help='Config file path.')

# Meta-Optimizer Options
parser.add('--meta-optimizer', type=str, metavar='STR', default='SGD',
           help='What meta optimizer to use for the current experiment, i.e., Adam, SGD')

# Optimizer options
parser.add('--optimizer', type=str, metavar='STR',
           help='What optimizer to use for the current experiment.')
parser.add('--grad-method', type=str, default='subgrad', metavar='STR',
           help='How to calculate gradients with respect to the objective func.')
parser.add('--cpu', action='store_true',
           help='Force to use CPU instead of GPU even if CUDA compatible GPU '
                'devices are available.')
parser.add('--device', type=str, default=None, help='cuda:0')
parser.add('--test', action='store_true', help='Run in test mode.')
parser.add('--state-scale', type=float, default=0.01, metavar='FLOAT',
           help='scale of the lstm states.')

# Optimizee general options
parser.add('--optimizee-type',
           choices=['QuadraticUnconstrained', 'LASSO',
                    'LogisticL1', 'LogisticL1CIFAR10', 'MnistCNN', 'MnistCNNMiniBatch'],
           help='Type of optimizees to be trained on.')
parser.add('--input-dim', type=int, metavar='INT',
           help='Dimension of the input (optimization variable).')
parser.add('--output-dim', type=int, metavar='INT',
           help='Dimension of the output (labels used to calculate loss).')
parser.add('--rho', type=float, default=0.1, metavar='FLOAT',
           help='Parameter for reg. term in the objective function.')
parser.add('--identical-dict', action='store_true',
           help='Use identical dictionaries (M) for the optimizees')
parser.add('--fixed-dict', action='store_true',
           help='Use a fixed dictionary for the optimizees')
parser.add('--sparsity', type=int, default=5, metavar='INT',
           help='Sparisty of the input variable.')
parser.add('--save-to-mat', action='store_true',
           help='save optmizees to mat file.')
parser.add('--optimizee-dir', type=str, metavar='STR',
           help='dir of optimizees.')
parser.add('--load-mat', action='store_true',
           help='load optmizees from mat file.')
parser.add('--save-sol', action='store_true',
           help='save solutions of optimizees.')
parser.add('--load-sol', action='store_true',
           help='save solutions of optimizees.')
parser.add('--ood', action='store_true',
           help='if use ood problem in optimizee')
parser.add('--ood-s', type=float, default=0.0, metavar='FLOAT',
           help='OOD on s')
parser.add('--ood-t', type=float, default=0.0, metavar='FLOAT',
           help='OOD on t')

# Model parameters
parser.add('--lstm-layers', type=int, default=2, metavar='INT',
           help='Number of layers of the neural network.')
parser.add('--lstm-hidden-size', type=int, default=256, metavar='INT',
           help='Number of layers of the neural network.')

parser.add('--e', type=int, default=100, metavar='INT',
           help='Exponsion coefficient')

parser.add('--rnnprop-beta1', type=float, default=0.95, metavar='FLOAT',
           help='Adam hyperparameter for RNNprop.')
parser.add('--rnnprop-beta2', type=float, default=0.95, metavar='FLOAT',
           help='Adam hyperparameter for RNNprop.')

parser.add('--a-use', action='store_true',
           help='Use the bias terms generated by LSTM.')
parser.add('--a-scale', type=float, default=1.0, metavar='FLOAT',
           help='Scaling factor before the bias term')
parser.add('--a-scale-learned', action='store_true',
           help='Learn scaling factor before the bias term '
                'as a learnable parameter')
parser.add('--a-norm', type=str, choices=['eye', 'tanh', 'sigmoid4', 'sigmoid3', 'sigmoid', 'sigmoid1', 'sigmoid05', 'exp', 'softplus', 'tanh', 'Gaussian', 'Gaussian1'],
           help='Normalization applied to the bias terms before they are '
                'applied.')

parser.add('--p-use', action='store_true',
           help='Use the pre-conditioners generated by LSTM.')
parser.add('--p-scale', type=float, default=1.0, metavar='FLOAT',
           help='Scaling factor before the pre-conditioner.')
parser.add('--p-scale-learned', action='store_true',
           help='Learn scaling factor before the pre-conditioner '
                'as a learnable parameter')
parser.add('--p-norm', type=str, choices=['eye', 'sigmoid', 'exp', 'softplus', 'Gaussian', 'Gaussian1'],
           help='Normalization applied to the pre-conditioners before they are '
                'applied to the gradients.')

# Parameter of LISTA Series
parser.add('--w-shared', action='store_true',
           help='Whether to share the parameter W in LISTA and LISTA-CPSS')
parser.add('--s-shared', action='store_true',
           help='Whether to share the parameter S in LISTA')
parser.add('--theta-shared', action='store_true',
           help='Whether to share the parameter theta LISTA and in LISTA-CPSS')

parser.add('--rho', type=float, default=0.2, metavar='FLOAT',
           help='Parameter for reg. term in the objective function.')
parser.add('--lamb', type=float, default=0.4, metavar='FLOAT',
           help='Parameter for reg. term in the objective function.')
parser.add('--p', type=float, default=0.012, metavar='FLOAT',
           help='Percent of support selection in LISTA-CPSS')
parser.add('--max-p', type=float, default=0.13, metavar='FLOAT',
           help='Max percent of support selection in LISTA-CPSS')

# Parameters of classic optimizers
parser.add('--step-size', type=float, default=None, metavar='FLOAT',
           help='Step size for the classic optimizers')
parser.add('--momentum1', type=float, default=None, metavar='FLOAT',
           help='decay factor of 1st order momentum (adam)')
parser.add('--momentum2', type=float, default=None, metavar='FLOAT',
           help='decay factor of 2nd order momentum (adam)')
parser.add('--eps', type=float, default=None, metavar='FLOAT',
           help='epsilon on adam optimizer')
parser.add('--hyper-step', type=float, default=None, metavar='FLOAT',
           help='Hyper step size of AdamHD')

# Parameter of B, C Shrinking
parser.add('--B-step-size', type=str, default=None, metavar='STR',
           help='Step size for Parameter B to alleviate OOD')
parser.add('--C-step-size', type=str, default=None, metavar='STR',
           help='Step size for Parameter C to alleviate OOD')

# Data parameters
parser.add('--seed', type=int, default=118, metavar='INT',
           help='Random seed for reproducibility')

# Training parameters
# parser.add('--objective', type=str, default='GT', metavar='{OBJECTIVE,L2,L1,GT}',
#            help='Objective used for the training')
parser.add('--save-dir', type=str, default='temp', metavar='STR',
           help='Saving directory for saved checkpoints and logs')
parser.add('--ckpt-path', type=str, default=None, metavar='STR',
           help='Path to the checkpoint to be loaded.')
parser.add('--loss-save-path', type=str, default=None, metavar='STR',
           help='Path to save the testing losses.')

# Training
parser.add('--global-training-steps', type=int, default=1000,
           help='Total number of training steps considered.')
parser.add('--optimizer-training-steps', type=int, default=100,
           help='Total number of batches of optimizees generated for training.')
parser.add('--unroll-length', type=int, default=1000,
           help='Total number of training steps considered.')

parser.add('--train-batch-size', type=int, default=128, metavar='N',
           help='Batch size for training')
parser.add('--val-batch-size', type=int, default=256, metavar='N',
           help='Batch size for validation')
parser.add('--test-batch-size', type=int, default=None, metavar='N',
           help='Batch size for testing')
parser.add('--val-size', type=int, default=2048, metavar='N',
           help='Number of validation samples')
parser.add('--test-size', type=int, default=2048, metavar='N',
           help='Number of testing samples')

parser.add('--print-freq', type=int, default=20,
           help='Frequency of printing training information')
parser.add('--val-freq', type=int, default=200,
           help='Frequency of validation')
parser.add('--val-length', type=int, default=100,
           help='Total length of optimization during validation')
parser.add('--test-length', type=int, default=100,
           help='Total length of optimization during testing')

parser.add('--init-lr', type=float, default=0.1, metavar='FLOAT',
           help='Initial learning rate')
parser.add('--scheduler', type=str, default='constant', metavar='STR',
           help='Learning rate scheduler.')
parser.add('--best-wait', type=int, default=5, metavar='N',
           help='Wait time for better validation performance')

parser.add('--epochs', type=int, default=5, help='Epochs')

parser.add('--clip-grad', action='store_true', help='if clip gradient')

parser.add('--debug', action='store_true', help='if use tensorboard')

parser.add('--loss-func', type=str, metavar='STR',
           help='What loss function to use.')
